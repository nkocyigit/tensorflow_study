{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"text-align:justify\">\n",
       "<strong>Cross Entropy</strong>\n",
       "</p>\n",
       "<p style=\"text-align:justify\">\n",
       "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability\n",
       "value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
       "</p>\n",
       "<br>\n",
       "<p>\n",
       "$$J = - \\frac{1}{N}(\\sum_{i=1}^{N} \\mathbf{y_i} \\cdot log(\\mathbf{\\hat{y}_i}))$$\n",
       "</p>\n",
       "<p style=\"text-align:justify\">\n",
       "Suppose I build a NN for classification. The last layer is a dense layer with softmax activation. \n",
       "I have five different classes to classify. Suppose for a single training example, \n",
       "the true label is [1 0 0 0 0] while the predictions be [0.1  0.5  0.1  0.1  0.2]\n",
       "How would I calculate the cross entropy loss for this example?\n",
       "</p>\n",
       "<br>\n",
       "$$J(\\theta) = - \\frac{1}{5}[\\hspace{5px} \\mathbf{1} \\cdot log(\\mathbf{0.1}) + \\mathbf{0} \\cdot log(\\mathbf{0.5}) \n",
       "                + \\mathbf{0} \\cdot log(\\mathbf{0.1}) + \\mathbf{0} \\cdot log(\\mathbf{0.1}) \n",
       "                + \\mathbf{0} \\cdot log(\\mathbf{0.2})\\hspace{5px}]$$\n",
       "<br>\n",
       "<p style=\"text-align:justify\">\n",
       "That means, the loss would be same no matter if the predictions are [0.1,0.5,0.1,0.1,0.2] \n",
       "or [0.1,0.6,0.1,0.1,0.1]?\n",
       "Yes, this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. \n",
       "The value is independent of how the remaining probability is split between incorrect classes.\n",
       "\n",
       "You will often see this equation averaged over all examples as a cost function. It is not always strictly \n",
       "adhered to in descriptions, but usually a loss function is lower level and describes how a single instance \n",
       "or component determines an error value, whilst a cost function is higher level and describes how a complete \n",
       "system is evaluated for optimisation.\n",
       "</p>\n",
       "<p>\n",
       "Cross entropy is usually used when there are three or more possible outcomes/classes, if there are 2 classes log loss\n",
       "can be used.\n",
       "</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<p style=\"text-align:justify\">\n",
    "<strong>Cross Entropy</strong>\n",
    "</p>\n",
    "<p style=\"text-align:justify\">\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability\n",
    "value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "</p>\n",
    "<br>\n",
    "<p>\n",
    "$$J = - \\frac{1}{N}(\\sum_{i=1}^{N} \\mathbf{y_i} \\cdot log(\\mathbf{\\hat{y}_i}))$$\n",
    "</p>\n",
    "<p style=\"text-align:justify\">\n",
    "Suppose I build a NN for classification. The last layer is a dense layer with softmax activation. \n",
    "I have five different classes to classify. Suppose for a single training example, \n",
    "the true label is [1 0 0 0 0] while the predictions be [0.1  0.5  0.1  0.1  0.2]\n",
    "How would I calculate the cross entropy loss for this example?\n",
    "</p>\n",
    "<br>\n",
    "$$J(\\theta) = - \\frac{1}{5}[\\hspace{5px} \\mathbf{1} \\cdot log(\\mathbf{0.1}) + \\mathbf{0} \\cdot log(\\mathbf{0.5}) \n",
    "                + \\mathbf{0} \\cdot log(\\mathbf{0.1}) + \\mathbf{0} \\cdot log(\\mathbf{0.1}) \n",
    "                + \\mathbf{0} \\cdot log(\\mathbf{0.2})\\hspace{5px}]$$\n",
    "<br>\n",
    "<p style=\"text-align:justify\">\n",
    "That means, the loss would be same no matter if the predictions are [0.1,0.5,0.1,0.1,0.2] \n",
    "or [0.1,0.6,0.1,0.1,0.1]?\n",
    "Yes, this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. \n",
    "The value is independent of how the remaining probability is split between incorrect classes.\n",
    "\n",
    "You will often see this equation averaged over all examples as a cost function. It is not always strictly \n",
    "adhered to in descriptions, but usually a loss function is lower level and describes how a single instance \n",
    "or component determines an error value, whilst a cost function is higher level and describes how a complete \n",
    "system is evaluated for optimisation.\n",
    "</p>\n",
    "<p>\n",
    "Cross entropy is usually used when there are three or more possible outcomes/classes, if there are 2 classes log loss\n",
    "can be used.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685919371237\n",
      "0.706991474368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "ground_truth  = [1, 0, 0, 0, 0]\n",
    "predictions   = [0.1, 0.5, 0.1, 0.1, 0.2]\n",
    "predictions2  = [0.1, 0.6, 0.1, 0.1, 0.1]\n",
    "\n",
    "print(log_loss(ground_truth, predictions))\n",
    "print(log_loss(ground_truth, predictions2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
